{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "column_names = [\n",
    "    \"S. NO\", \"NAME OF RESERVOIR\", \"FRL (m)\", \"RESERVOIR LEVEL (m)\", \"LIVE CAPACITY AT FRL (BCM)\", \n",
    "    \"CURRENT LIVE STORAGE (BCM)\", \"DATE\", \"CURRENT YEAR\", \"LAST YEAR\", \"LAST 10 YEARS AVERAGE\", \n",
    "    \"IRR. (CCA) IN TH. HA\", \"HYDEL IN MW\"\n",
    "]\n",
    "\n",
    "txt_folder = \"./text\"\n",
    "excel_output = \"./excel-dir\"\n",
    "!rm -rf excel_output\n",
    "os.makedirs(excel_output, exist_ok=True)\n",
    "\n",
    "# pattern = re.compile(\n",
    "#     r'^\\*?(\\d+)\\s+([A-Z\\(\\)a-z\\s]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+(\\d{2}-\\d{2}-\\d{4})\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.-]+)\\s+([\\d.-]+)'\n",
    "# )\n",
    "\n",
    "# pattern = re.compile(\n",
    "#     r'(\\d+)\\s+(.+?)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+(\\d{2}-\\d{2}-\\d{4})\\s+([\\d.]+|\\s+)\\s+([\\d.]+|\\s+)\\s+([\\d.]+|\\s+)\\s+([\\d.-]+|\\s+)\\s+([\\d.-]+|\\s+)'\n",
    "# )\n",
    "pattern = re.compile(\n",
    "    r'^([*,&,#,@,!,$,%,^,(,),_,+]*\\d+)\\s+(.+?)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+(\\d{2}-\\d{2}-\\d{2,4})\\s+([\\d.]+|\\s+)\\s+([\\d.]+|\\s+)\\s+([\\d.]+|\\s+)\\s+([\\d.-]+|\\s+)\\s+([\\d.-]+|\\s+)'\n",
    ")\n",
    "\n",
    "for idx,filename in enumerate(os.listdir(txt_folder)):\n",
    "    # if(idx>0):\n",
    "    #     break\n",
    "    filepath = os.path.join(txt_folder, filename)\n",
    "    excelpath = os.path.join(excel_output, filename.replace(\".txt\",\".csv\"))\n",
    "    extracted_lines = []\n",
    "    with open(filepath, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                match = pattern.match(line)\n",
    "                if match:\n",
    "                    extracted_lines.append(match.groups())\n",
    "    with open(excelpath, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        csvwriter.writerow(column_names)\n",
    "        csvwriter.writerows(extracted_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "csv_folder = \"./excel-dir\"\n",
    "unique_reservoirs = set()\n",
    "\n",
    "for filename in os.listdir(csv_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        filepath = os.path.join(csv_folder, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        reservoir_names = df[\"NAME OF RESERVOIR\"].astype(str)  # Convert to string to handle mixed data types\n",
    "\n",
    "        for name in reservoir_names:\n",
    "            unique_reservoirs.add(name)\n",
    "\n",
    "print(\"Unique reservoir names found (excluding numbers):\")\n",
    "display(len(unique_reservoirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservoir_list = ['ALIYAR',\n",
    " 'ALMATTI',\n",
    " 'ATAL SAGAR(MADIKHEDA)',\n",
    " 'BALIMELA',\n",
    " 'BANSAGAR',\n",
    " 'BARGI',\n",
    " 'BARNA DAM',\n",
    " 'BARVI DAM',\n",
    " 'BHADAR',\n",
    " 'BHADRA',\n",
    " 'BHAMA ASKHED',\n",
    " 'BHANDARDARA',\n",
    " 'BHATGHAR',\n",
    " 'BHATSA',\n",
    " 'BHIMA(UJJANI)',\n",
    " 'BISALPUR',\n",
    " 'BRAHMANI(GUJ)',\n",
    " 'CHANDAN DAM',\n",
    " 'CHASKAMAN',\n",
    " 'DAMANGANGA',\n",
    " 'DANTIWADA',\n",
    " 'DARNA DAM',\n",
    " 'DHOM',\n",
    " 'DIMBHE DAM',\n",
    " 'DONKARAYI',\n",
    " 'DOYANG HEP',\n",
    " 'DUDHAWA',\n",
    " 'DUDHGANGA',\n",
    " 'GANDHI SAGAR',\n",
    " 'GERUSOPPA',\n",
    " 'GETALSUD',\n",
    " 'GHATAPRABHA(HIDKAL)',\n",
    " 'GIRNA',\n",
    " 'GOBIND SAGAR(BHAKRA)',\n",
    " 'GUMTI',\n",
    " 'HARANGI',\n",
    " 'HARIHARJHOR',\n",
    " 'HATHMATI',\n",
    " 'HEMAVATHY',\n",
    " 'HIRAKUD',\n",
    " 'IDAMALAYAR',\n",
    " 'IDUKKI',\n",
    " 'INDIRA SAGAR',\n",
    " 'ISAPUR',\n",
    " 'JAISAMAND',\n",
    " 'JAWAI DAM',\n",
    " 'JAYAKWADI(PAITHAN)',\n",
    " 'JHAKAM',\n",
    " 'JIRGO',\n",
    " 'KABINI',\n",
    " 'KADANA',\n",
    " 'KADDAM (K.N.R.)',\n",
    " 'KAKKI',\n",
    " 'KALLADA(PARAPPAR)',\n",
    " 'KANDALERU',\n",
    " 'KANGSABATI',\n",
    " 'KANHER',\n",
    " 'KARJAN',\n",
    " 'KHADAKVASLA',\n",
    " 'KHANDONG',\n",
    " 'KOL DAM',\n",
    " 'KOLAR DAM',\n",
    " 'KONAR',\n",
    " 'KOYANA',\n",
    " 'KRISHNARAJA SAGARA',\n",
    " 'LINGANAMAKKI',\n",
    " 'LOWER BHAWANI',\n",
    " 'LOWER MANAIR',\n",
    " 'MACHCHHU-I',\n",
    " 'MACHCHHU-II',\n",
    " 'MACHKUND(JALAPUT)',\n",
    " 'MAHANADI',\n",
    " 'MAHI BAJAJ SAGAR',\n",
    " 'MAITHON',\n",
    " 'MALAMPUZHA',\n",
    " 'MALAPRABHA(RENUKA)',\n",
    " 'MANDIRA DAM',\n",
    " 'MANI DAM',\n",
    " 'MANIKDOH',\n",
    " 'MATATILA',\n",
    " 'MAUDAHA',\n",
    " 'MAYURAKSHI',\n",
    " 'MEJA',\n",
    " 'METTUR(STANLEY)',\n",
    " 'MINIMATA BANGO',\n",
    " 'MULA',\n",
    " 'MULSHI',\n",
    " 'MUSI PROJECT',\n",
    " 'NAGARJUNA SAGAR',\n",
    " 'NANAK SAGAR',\n",
    " 'NARAYANPUR',\n",
    " 'NIRA DEOGHAR',\n",
    " 'NIZAM SAGAR',\n",
    " 'OMKARESHWAR',\n",
    " 'PANAM',\n",
    " 'PANCHET HILL',\n",
    " 'PANSHET(TANAJISAGAR)',\n",
    " 'PARAMBIKULAM',\n",
    " 'PENCH(TOTLADOH)',\n",
    " 'PERIYAR',\n",
    " 'PONG DAM(BEAS)',\n",
    " 'PRIYADARSHINI JURALA',\n",
    " 'RAJGHAT DAM',\n",
    " 'RAMGANGA',\n",
    " 'RANA PRATAP SAGAR',\n",
    " 'RANGAWAN',\n",
    " 'RENGALI',\n",
    " 'RIHAND',\n",
    " 'SABARMATI(DHAROI)',\n",
    " 'SALANADI',\n",
    " 'SANJAY SAROVAR',\n",
    " 'SAPUA',\n",
    " 'SARDAR SAROVAR',\n",
    " 'SATHANUR',\n",
    " 'SHARDA SAGAR',\n",
    " 'SHETRUNJI',\n",
    " 'SHOLAYAR',\n",
    " 'SINGUR',\n",
    " 'SIRSI',\n",
    " 'SOMASILA',\n",
    " 'SRIRAMSAGAR',\n",
    " 'SRISAILAM',\n",
    " 'SUKHI(GUJ)',\n",
    " 'SUPA',\n",
    " 'SURYA',\n",
    " 'TANDULA',\n",
    " 'TATTIHALLA',\n",
    " 'TAWA',\n",
    " 'TEHRI',\n",
    " 'TENUGHAT',\n",
    " 'THEIN DAM',\n",
    " 'THOKARWADI',\n",
    " 'TILAIYA',\n",
    " 'TILLARI',\n",
    " 'TUNGABHADRA',\n",
    " 'UKAI',\n",
    " 'UMRONG',\n",
    " 'UND-I',\n",
    " 'UPPER INDRAVATI',\n",
    " 'UPPER KOLAB',\n",
    " 'UPPER TAPI',\n",
    " 'UPPER VAITARNA',\n",
    " 'UPPER WARDHA',\n",
    " 'URMODI',\n",
    " 'VAIGAI',\n",
    " 'VANI VILAS SAGAR',\n",
    " 'VEER DAM',\n",
    " 'WATRAK',\n",
    " 'YELDARI',\n",
    " 'YELERU']\n",
    "\n",
    "reservoir_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Latitude and Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from time import sleep\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"reservoir_locator\")\n",
    "\n",
    "# Function to get latitude and longitude\n",
    "def get_lat_long(reservoir_name):\n",
    "    try:\n",
    "        location = geolocator.geocode(reservoir_name)\n",
    "        if location:\n",
    "            return location.latitude, location.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "# List to store data\n",
    "data = []\n",
    "\n",
    "# Get coordinates for each reservoir\n",
    "for reservoir in reservoir_list:\n",
    "    lat, longitude = get_lat_long(reservoir)\n",
    "    data.append([reservoir, lat, longitude])\n",
    "    sleep(1)  # To prevent overwhelming the geocoding service\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"NAME OF RESERVOIR\", \"Latitude\", \"Longitude\"])\n",
    "df.to_csv(\"./lat-long.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf reservoir_data final_reservoir_data weekwise_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the reservoir data and creating unique files\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "csv_folder = \"./excel-dir\"\n",
    "output_folder = \"reservoir_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Load latitude and longitude data\n",
    "lat_long_df = pd.read_csv(\"./lat-long.csv\")  # Assuming columns are 'NAME OF RESERVOIR', 'Latitude', 'Longitude'\n",
    "\n",
    "for filename in os.listdir(csv_folder):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        filepath = os.path.join(csv_folder, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        df['DATE'] = pd.to_datetime(df['DATE'], format='%d-%m-%Y')\n",
    "\n",
    "        for reservoir in reservoir_list:\n",
    "            reservoir_data = df[df[\"NAME OF RESERVOIR\"] == reservoir].copy()\n",
    "            reservoir_data.sort_values(by=\"DATE\", inplace=True)\n",
    "            reservoir_data.drop(columns=[\"S. NO\"], inplace=True)  \n",
    "\n",
    "            if not reservoir_data.empty:\n",
    "                reservoir_data = reservoir_data.merge(\n",
    "                    lat_long_df, on=\"NAME OF RESERVOIR\", how=\"left\"\n",
    "                )\n",
    "\n",
    "                cleaned_name = re.sub(r'[.\\s]+', '', reservoir) \n",
    "                output_path = os.path.join(output_folder, f\"{cleaned_name}.csv\")\n",
    "\n",
    "                if os.path.exists(output_path):\n",
    "                    reservoir_data.to_csv(output_path, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    reservoir_data.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding weeks\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "reservoir_dir = \"./reservoir_data\"\n",
    "updated_reservoir_data = \"./final_reservoir_data\"\n",
    "\n",
    "os.makedirs(updated_reservoir_data, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(reservoir_dir):\n",
    "    filepath = os.path.join(reservoir_dir, filename)\n",
    "    outputPath = os.path.join(updated_reservoir_data, filename)\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df['WEEK'] = df['DATE'].dt.isocalendar().week\n",
    "\n",
    "    # Get the reservoir name \n",
    "    reservoir_name = df['NAME OF RESERVOIR'].iloc[0]\n",
    "    latitude = df[\"Latitude\"].iloc[0]\n",
    "    longitude = df[\"Longitude\"].iloc[0]\n",
    "\n",
    "    # Calculate weekly mean and std dev of \"RESERVOIR LEVEL (m)\"\n",
    "    weekly_stats = df.groupby('WEEK')['RESERVOIR LEVEL (m)'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    # Rename columns for clarity\n",
    "\n",
    "    # Add the reservoir name column\n",
    "    weekly_stats.columns = ['WEEK', 'MEAN', 'STD']\n",
    "    weekly_stats['NAME OF RESERVOIR'] = reservoir_name\n",
    "    weekly_stats['Latitude'] = latitude\n",
    "    weekly_stats['Longitude'] = longitude\n",
    "\n",
    "    # Save the resulting DataFrame to a new CSV file\n",
    "    weekly_stats.to_csv(outputPath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating Weekly Data\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "final_reservoir_data = \"./final_reservoir_data\"\n",
    "weekly_reservoir_data = \"./weekwise_data\"\n",
    "\n",
    "os.makedirs(weekly_reservoir_data, exist_ok=True)\n",
    "\n",
    "columns_to_keep = ['NAME OF RESERVOIR', 'Latitude', 'Longitude', 'MEAN', 'STD']\n",
    "\n",
    "for week in range(1, 54):\n",
    "    outPath = os.path.join(weekly_reservoir_data, f\"{week}.csv\")\n",
    "\n",
    "    # Create a new empty DataFrame for each week\n",
    "    weekly_df = pd.DataFrame(columns=columns_to_keep)\n",
    "\n",
    "    # Flag to check if the header has been written\n",
    "    header_written = False \n",
    "\n",
    "    for filename in os.listdir(final_reservoir_data):\n",
    "        filepath = os.path.join(final_reservoir_data, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        week_data = df[df['WEEK'] == week][columns_to_keep]\n",
    "\n",
    "        # Append data to the weekly DataFrame\n",
    "        weekly_df = pd.concat([weekly_df, week_data])\n",
    "        # Write header only once for each week file\n",
    "        if not header_written and not week_data.empty:\n",
    "            weekly_df.to_csv(outPath, mode='a', header=True, index=False)\n",
    "            header_written = True\n",
    "        else:\n",
    "            week_data.to_csv(outPath, mode='a', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "for filename in os.listdir(\"weekwise_data\"):\n",
    "    filepath = os.path.join(\"weekwise_data\", filename)\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    df.sort_values(by=\"NAME OF RESERVOIR\", inplace=True)\n",
    "\n",
    "    # Overwrite the existing CSV file with the sorted data\n",
    "    df.to_csv(filepath, index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making new data workable\n",
    "import re\n",
    "import csv\n",
    "\n",
    "column_names = [\n",
    "    \"S. NO\", \"NAME OF RESERVOIR\", \"FRL (m)\", \"RESERVOIR LEVEL (m)\", \"LIVE CAPACITY AT FRL (BCM)\", \n",
    "    \"CURRENT LIVE STORAGE (BCM)\", \"DATE\", \"CURRENT YEAR\", \"LAST YEAR\", \"LAST 10 YEARS AVERAGE\", \n",
    "    \"IRR. (CCA) IN TH. HA\", \"HYDEL IN MW\"\n",
    "]\n",
    "pattern = re.compile(\n",
    "    r'^([*,&,#,@,!,$,%,^,(,),_,+]*\\d+)\\s+(.+?)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+([\\d.]+)\\s+(\\d{2}-\\d{2}-\\d{2,4})\\s+([\\d.]+|\\s+)\\s+([\\d.]+|\\s+)\\s+([\\d.]+|\\s+)\\s+([\\d.-]+|\\s+)\\s+([\\d.-]+|\\s+)'\n",
    ")\n",
    "\n",
    "extracted_lines = []\n",
    "with open(\"newdata.txt\", 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            match = pattern.match(line)\n",
    "            if match:\n",
    "                extracted_lines.append(match.groups())\n",
    "with open(\"newdata.csv\", 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(column_names)\n",
    "    csvwriter.writerows(extracted_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"newdata.csv\")\n",
    "z = True\n",
    "x = df['NAME OF RESERVOIR'].tolist()\n",
    "for i in x:\n",
    "    if i not in reservoir_list:\n",
    "        z = False\n",
    "columns_to_drop = ['S. NO', 'FRL (m)',\n",
    "       'LIVE CAPACITY AT FRL (BCM)', 'CURRENT LIVE STORAGE (BCM)', 'DATE',\n",
    "       'CURRENT YEAR', 'LAST YEAR', 'LAST 10 YEARS AVERAGE',\n",
    "       'IRR. (CCA) IN TH. HA', 'HYDEL IN MW', ]\n",
    "\n",
    "if z :\n",
    "    lat_long_df = pd.read_csv(\"lat-long.csv\")\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'],format=\"%d-%m-%Y\")\n",
    "    df['WEEK'] = df['DATE'].dt.isocalendar().week\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    df.columns = [\"NAME OF RESERVOIR\", 'CURRENT','WEEK']\n",
    "    df.to_csv(\"processed_new_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating mapping_data.csv with historical mean and current data\n",
    "import pandas as pd\n",
    "\n",
    "new_df = pd.read_csv(\"processed_new_data.csv\")\n",
    "week = new_df[\"WEEK\"][0]\n",
    "\n",
    "historical_df = pd.read_csv(f\"./weekwise_data/{week}.csv\")\n",
    "\n",
    "map_df = historical_df.merge(new_df, on=\"NAME OF RESERVOIR\", how=\"left\")\n",
    "map_df.drop(columns=\"WEEK\", inplace=True)\n",
    "map_df.columns = ['NAME OF RESERVOIR', 'Latitude', 'Longitude', 'Historical Mean','Historical STD',\n",
    "       'Current']\n",
    "map_df['Historical STD'].fillna(0, inplace=True)\n",
    "map_df.to_csv(\"map_data.csv\", index=False)\n",
    "map_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping DHOM due to missing location data\n",
      "Skipping GIRNA due to missing location data\n",
      "Skipping GOBIND SAGAR(BHAKRA) due to missing location data\n",
      "Skipping HARIHARJHOR due to missing location data\n",
      "Skipping KADANA due to missing location data\n",
      "Skipping KAKKI due to missing location data\n",
      "Skipping KOLAR DAM due to missing location data\n",
      "Skipping KOYANA due to missing location data\n",
      "Skipping MACHCHHU-I due to missing location data\n",
      "Skipping MACHCHHU-II due to missing location data\n",
      "Skipping MINIMATA BANGO due to missing location data\n",
      "Skipping MULA due to missing location data\n",
      "Skipping PANAM due to missing location data\n",
      "Skipping PANSHET(TANAJISAGAR) due to missing location data\n",
      "Skipping PENCH(TOTLADOH) due to missing location data\n",
      "Skipping PONG DAM(BEAS) due to missing location data\n",
      "Skipping SABARMATI(DHAROI) due to missing location data\n",
      "Skipping SALANADI due to missing location data\n",
      "Skipping SUKHI(GUJ) due to missing location data\n",
      "Skipping SUPA due to missing location data\n",
      "Skipping SURYA due to missing location data\n",
      "Skipping THOKARWADI due to missing location data\n",
      "Skipping UND-I due to missing location data\n",
      "Skipping UPPER INDRAVATI due to missing location data\n",
      "Skipping UPPER TAPI due to missing location data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "import branca\n",
    "\n",
    "reservoir_data = {}  \n",
    "df = pd.read_csv(\"map_data.csv\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    reservoir_name = row[\"NAME OF RESERVOIR\"]\n",
    "    latitude = row[\"Latitude\"]\n",
    "    longitude = row[\"Longitude\"]\n",
    "    if pd.isna(latitude) or pd.isna(longitude):\n",
    "        print(f\"Skipping {reservoir_name} due to missing location data\")\n",
    "        continue\n",
    "\n",
    "    current_level = row['Current']\n",
    "    historical_mean = row['Historical Mean']\n",
    "    historical_std = row['Historical STD']\n",
    "    z_score = 0 if historical_std == 0 else (current_level - historical_mean) / historical_std\n",
    "    anomaly = \"Low\" if z_score < -1 else \"High\" if z_score > 1 else \"Normal\"\n",
    "    reservoir_data[reservoir_name] = {\n",
    "        'Latitude': latitude,\n",
    "        'Longitude': longitude,\n",
    "        'RESERVOIR LEVEL (m)': current_level,\n",
    "        'Anomaly': anomaly\n",
    "    }\n",
    "\n",
    "center_lat = sum(data['Latitude'] for data in reservoir_data.values()) / len(reservoir_data)\n",
    "center_lon = sum(data['Longitude'] for data in reservoir_data.values()) / len(reservoir_data)\n",
    "map_center = [center_lat, center_lon]\n",
    "\n",
    "# Create a Folium map restricted to India\n",
    "m = folium.Map(location=map_center, zoom_start=5, tiles='cartodb positron')\n",
    "m.fit_bounds([[8.4, 68.7], [37.6, 97.25]])  # Bounding box for India\n",
    "\n",
    "# Color mapping for anomalies\n",
    "anomaly_colors = {\n",
    "    \"Normal\": \"green\",\n",
    "    \"Low\": \"red\",\n",
    "    \"High\": \"blue\"\n",
    "}\n",
    "\n",
    "# Add markers to the map\n",
    "for name, data in reservoir_data.items():\n",
    "    popup_content = f\"\"\"\n",
    "    <div style=\"font-size: 14px;\">\n",
    "        <b>{name}</b><br>\n",
    "        <b>Reservoir Level:</b> {data['RESERVOIR LEVEL (m)']} m<br>\n",
    "        <b>Anomaly:</b> <span style=\"color:{anomaly_colors[data['Anomaly']]}\">{data['Anomaly']}</span>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    folium.CircleMarker(\n",
    "        location=[data['Latitude'], data['Longitude']],\n",
    "        radius=7,\n",
    "        color=anomaly_colors[data['Anomaly']],\n",
    "        fill=True,\n",
    "        fill_color=anomaly_colors[data['Anomaly']],\n",
    "        fill_opacity=0.7,\n",
    "        popup=folium.Popup(popup_content, max_width=300)\n",
    "    ).add_to(m)\n",
    "\n",
    "# Add a legend\n",
    "legend_html = '''\n",
    "     <div style=\"position: fixed; \n",
    "     bottom: 50px; left: 50px; width: 150px; height: 90px; \n",
    "     border:2px solid grey; z-index:9999; font-size:14px;\n",
    "     background-color:white; opacity: 0.8;\">\n",
    "     &nbsp; <b>Legend</b> <br>\n",
    "     &nbsp; <i class=\"fa fa-circle\" style=\"color:green\"></i> Normal<br>\n",
    "     &nbsp; <i class=\"fa fa-circle\" style=\"color:red\"></i> Low<br>\n",
    "     &nbsp; <i class=\"fa fa-circle\" style=\"color:blue\"></i> High<br>\n",
    "     </div>\n",
    "     '''\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Save the map\n",
    "m.save('reservoir_map.html')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
